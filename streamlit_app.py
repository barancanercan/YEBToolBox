import streamlit as st
from datetime import datetime, time, timedelta, date
import pandas as pd
from urllib.parse import urlparse
from app.scraper import NewsSiteConfig, UniversalNewsScraper
import io
from app.streamlit_trend_app import run_trends_app


def get_site_config(url: str) -> NewsSiteConfig:
    # Bu fonksiyon, verilen URL'ye gÃ¶re uygun NewsSiteConfig'i dÃ¶ndÃ¼rÃ¼r.
    # GerÃ§ek bir uygulamada, bu kÄ±sÄ±m bir veritabanÄ±ndan veya yapÄ±landÄ±rma dosyasÄ±ndan
    # siteye Ã¶zgÃ¼ seÃ§icileri Ã§ekecek ÅŸekilde geniÅŸletilebilir.
    # Åimdilik, sadece Hurriyet iÃ§in sabit bir konfigÃ¼rasyon dÃ¶ndÃ¼rÃ¼yoruz.
    # Gelecekte, burada daha akÄ±llÄ± bir URL analizi veya kullanÄ±cÄ±dan seÃ§ici giriÅŸi eklenebilir.

    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    if "hurriyet.com.tr" in domain:
        return NewsSiteConfig(
            base_url=f"https://{domain}",
            listing_page_paths=["/gundem/", "/", "/son-dakika/"],
            article_link_selectors=[
                'a[href*="/gundem/"]',
                'a[href*="/haber/"]',
                '.news-item a',
                '.article-link'
            ],
            title_selectors=['h1', '.news-title', '.article-title'],
            content_selectors=[
                '.news-content',
                '.article-content',
                '.content',
                '.news-text',
                'div[data-news-content]'
            ],
            date_selectors=[
                'time[datetime]',
                '.news-datetime',
                '.article-date',
                '[data-date]',
                '.date-time'
            ],
            turkish_date_parsing_enabled=True
        )
    elif "ntv.com.tr" in domain:
        return NewsSiteConfig(
            base_url=f"https://{domain}",
            listing_page_paths=["/", "/son-dakika", "/turkiye", "/dunya"], # NTV iÃ§in ana sayfalar
            article_link_selectors=[
                'a[data-story-channel="headline"]',
                'li.related-news-item a.card-link',
                'h3.ntv-main-slider-item-first-title a',
                'a[href*=".ntv.com.tr/"]' # Daha genel bir link seÃ§ici
            ],
            title_selectors=['h1', 'meta[property="og:title"]', 'meta[name="title"]'],
            content_selectors=['div.category-detail-content', 'div[itemprop="articleBody"]', 'div#contentBodyArea'],
            date_selectors=['meta[name="datePublished"]', 'span.date', 'time', '.pubdate'],
            turkish_date_parsing_enabled=False # NTV ISO formatÄ±nÄ± kullandÄ±ÄŸÄ± iÃ§in
        )
    else:
        # VarsayÄ±lan veya genel bir konfigÃ¼rasyon, Ã¶zelleÅŸtirme gerekebilir
        st.warning(f"\'{url}\' iÃ§in Ã¶zel bir yapÄ±landÄ±rma bulunamadÄ±. Genel seÃ§iciler denenecektir. \n\n**Not:** Bu sitenin doÄŸru Ã§alÄ±ÅŸmasÄ± iÃ§in \'scraper.py\' dosyasÄ±nda Ã¶zel CSS seÃ§icileri tanÄ±mlamanÄ±z gerekebilir.")
        return NewsSiteConfig(
            base_url=url,
            listing_page_paths=["/"] ,
            article_link_selectors=['a[href]', '.news-link a', '.article-card a', '.article-item a', '.post-link'],
            title_selectors=['h1', 'h2.title', '.article-title', 'meta[property="og:title"]', 'meta[name="title"]'],
            content_selectors=['div.content-body', '.article-content', 'div[itemprop="articleBody"]', 'div.entry-content', 'div.single-post-content'],
            date_selectors=['time', '.date', '.pubdate', '[data-timestamp]', 'span.post-date', 'div.date-time'],
            turkish_date_parsing_enabled=False # VarsayÄ±lan olarak TÃ¼rkÃ§e olmayan siteler iÃ§in False
        )

# Ana uygulama mantÄ±ÄŸÄ±
def main():
    st.set_page_config(
        page_title="YEB Uygulama PortalÄ±", # Ana sayfanÄ±n baÅŸlÄ±ÄŸÄ±
        page_icon="â­",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.sidebar.title("Uygulama SeÃ§imi")
    app_mode = st.sidebar.radio(
        "LÃ¼tfen bir uygulama seÃ§in:",
        ("Haber Scraper", "Google Trends Analizi")
    )

    if app_mode == "Haber Scraper":
        run_news_scraper_app()
    elif app_mode == "Google Trends Analizi":
        run_trends_app()

def run_news_scraper_app():
    st.title("ğŸ“° YEB Haber Scraper")
    st.markdown("Haber sitelerinden belirli tarih aralÄ±klarÄ±ndaki makaleleri Ã§ekin ve indirin.")

    # Session state'i baÅŸlat
    if 'news_df' not in st.session_state:
        st.session_state['news_df'] = pd.DataFrame()
    if 'button_clicked' not in st.session_state:
        st.session_state['button_clicked'] = False

    # --- KullanÄ±cÄ± GiriÅŸleri ---

    st.header("1. Haber Sitesi Linki")
    news_site_url = st.text_input(
        "LÃ¼tfen haber sitesinin URL'sini girin (Ã¶rneÄŸin: https://www.hurriyet.com.tr)",
        "https://www.hurriyet.com.tr"
    )

    st.header("2. Tarih ve Saat AralÄ±ÄŸÄ±")

    col1, col2 = st.columns(2)

    with col1:
        start_date = st.date_input("BaÅŸlangÄ±Ã§ Tarihi", datetime.now() - timedelta(days=1))
        start_time_input = st.time_input("BaÅŸlangÄ±Ã§ Saati", time(0, 0))

    with col2:
        end_date = st.date_input("BitiÅŸ Tarihi", datetime.now())
        end_time_input = st.time_input("BitiÅŸ Saati", time(23, 59))

    start_datetime = datetime.combine(start_date, start_time_input)
    end_datetime = datetime.combine(end_date, end_time_input)

    # Hata ve durum mesajlarÄ± iÃ§in yer tutucular
    error_placeholder = st.empty()
    status_placeholder = st.empty()

    if start_datetime >= end_datetime:
        error_placeholder.error("BaÅŸlangÄ±Ã§ tarihi ve saati, bitiÅŸ tarihinden ve saatinden Ã¶nce olmalÄ±dÄ±r.")

    # --- Haber Ã‡ekme Butonu ---
    st.markdown("---")

    if not st.session_state['button_clicked']:
        if st.button("Haberleri Ã‡ek", type="primary", key='fetch_news_button'):
            if start_datetime < end_datetime:
                error_placeholder.empty() # Ã–nceki hatayÄ± temizle
                status_placeholder.info("Haberler Ã§ekiliyor, lÃ¼tfen bekleyin...")
                st.session_state['button_clicked'] = True # Butona tÄ±klandÄ±ÄŸÄ±nÄ± iÅŸaretle
                
                config = get_site_config(news_site_url)
                scraper = UniversalNewsScraper(config)
                
                # Ä°lerleme raporlama fonksiyonu
                def update_status(message):
                    status_placeholder.info(message)

                with st.spinner('Haberler Ã§ekiliyor...'):
                    news_data = scraper.scrape_news_by_time_range(start_datetime, end_datetime, max_listing_pages=2, status_callback=update_status)
                
                if news_data:
                    st.success(f"âœ“ {len(news_data)} haber bulundu!")
                    df = pd.DataFrame(news_data)
                    st.session_state['news_df'] = df
                    st.rerun() # DataFrame gÃ¼ncellendiÄŸinde uygulamayÄ± yeniden Ã§alÄ±ÅŸtÄ±r
                else:
                    st.warning("Belirtilen kriterlere uygun haber bulunamadÄ±.")
                    st.session_state['button_clicked'] = False # Haber bulunamazsa butonu tekrar gÃ¶ster
            else:
                error_placeholder.error("LÃ¼tfen geÃ§erli bir tarih aralÄ±ÄŸÄ± seÃ§in.")
                st.session_state['button_clicked'] = False # Hata olursa butonu tekrar gÃ¶ster

    # EÄŸer haberler Ã§ekildiyse, sonuÃ§larÄ± ve indirme butonlarÄ±nÄ± gÃ¶ster
    if not st.session_state['news_df'].empty:
        st.header("3. Ã‡ekilen Haberler")
        st.dataframe(st.session_state['news_df'])

        st.subheader("4. SonuÃ§larÄ± Ä°ndir")
        df_to_download = st.session_state['news_df']

        col_csv, col_excel = st.columns(2)

        with col_csv:
            csv_buffer = io.StringIO()
            df_to_download.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
            st.download_button(
                label="CSV Olarak Ä°ndir",
                data=csv_buffer.getvalue(),
                file_name=f"yeb_haberler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                key='download_csv'
            )

        with col_excel:
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                df_to_download.to_excel(writer, index=False, sheet_name='Haberler')
            excel_buffer.seek(0)
            st.download_button(
                label="Excel Olarak Ä°ndir",
                data=excel_buffer.getvalue(),
                file_name=f"yeb_haberler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key='download_excel'
            )

    # EÄŸer yeniden Ã§ekmek isterse butonu tekrar gÃ¶ster
    if st.session_state['button_clicked'] and not st.session_state['news_df'].empty:
        st.markdown("---")
        if st.button("Yeni Arama Yap", key='new_search_button'):
            st.session_state['news_df'] = pd.DataFrame() # Mevcut veriyi temizle
            st.session_state['button_clicked'] = False # Butonu tekrar gÃ¶ster
            st.rerun()

if __name__ == "__main__":
    main() 